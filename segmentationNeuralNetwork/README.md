# Segmentation Neural Network

A semantic segmentation neural network designed for real-time driving applications using Unity-generated data.

## ğŸ¯ Overview

This segmentation neural network processes Unity driving simulation data to perform semantic segmentation with 12 classes optimized for autonomous driving scenarios. The system includes stable training scripts, real-time inference capabilities, and comprehensive testing tools.

## ğŸ“¦ Pre-trained Models

### Stable Model Checkpoint
The best performing stable model checkpoint is available for download:

**ğŸ“ Best Stable Model**: [Download from Google Drive](https://drive.google.com/file/d/1_Zmd_MHT9rcCJjkfZHsx-8saL2voLyut/view?usp=share_link)

- **Architecture**: MobileNetV2 UNet
- **Training**: Stable training with overfitting prevention
- **Performance**: 97.0% pixel accuracy, 0.298 mean IoU
- **Speed**: ~47.0 FPS on Apple Silicon
- **Location**: Place in `checkpoints_stable/best_model.pth`

### Fast Model Checkpoint
The speed-optimized fast model checkpoint is available for download:

**âš¡ Best Fast Model**: [Download from Google Drive](https://drive.google.com/file/d/1NaC-fEULmcQlA5oTIsxPHgoP9ZPV6Ati/view?usp=share_link)

- **Architecture**: EfficientNet-B0 UNet with reduced decoder
- **Training**: Speed-optimized with memory efficiency
- **Performance**: 48.1 FPS with 0.1337 validation loss
- **Speed**: ~48.1 FPS on Apple Silicon (excellent for real-time driving)
- **Location**: Place in `checkpoints_fast/best_model.pth`

## ğŸš€ Quick Start

### 1. Download Pre-trained Model
```bash
# Download the stable model (high accuracy) and place it at:
segmentationNeuralNetwork/checkpoints_stable/best_model.pth

# OR download the fast model (high speed) and place it at:
segmentationNeuralNetwork/checkpoints_fast/best_model.pth
```

### 2. Test the Model
```bash
# Interactive demo with visualization (auto-detects model type)
python demo_segmentation.py                              # Uses stable model by default
python demo_segmentation.py --model checkpoints_fast/best_model.pth  # Uses fast model

# Performance/speed test for driving assessment
python demo_segmentation.py --performance_test           # Test stable model speed
python demo_segmentation.py --model checkpoints_fast/best_model.pth --performance_test  # Test fast model speed

# Batch performance analysis
python demo_segmentation.py --batch_analysis --num_samples 10
```

### 3. Real-time Performance Test
```bash
# Command-line performance benchmarking
python test_stable_model.py --quick        # Quick 5-sample test
python test_stable_model.py --real-time    # 30-second FPS test
```

## ğŸ“Š Model Performance

### Stable Model (MobileNetV2)
The stable model demonstrates:
- **Pixel Accuracy**: 97.0% 
- **Mean IoU**: 0.298
- **Validation Loss**: 0.1159
- **Inference Speed**: 47.0 FPS (Apple Silicon MPS)
- **Best Classes**: Void, Building, Wall detection
- **Challenging Classes**: Road, Vehicle, Pedestrian detection

### Fast Model (EfficientNet-B0)
The fast model demonstrates:
- **Validation Loss**: 0.1337
- **Inference Speed**: 48.1 FPS (Apple Silicon MPS) - **EXCELLENT for real-time driving**
- **Latency**: 20.8ms average response time
- **Consistency**: Very stable performance (0.9ms std dev)
- **Real-time Assessment**: âœ… Exceeds 30+ FPS requirement for autonomous driving

## ğŸ—ï¸ Architecture

### Stable Model
- **Encoder**: MobileNetV2 (ImageNet pre-trained)
- **Decoder**: UNet architecture (default channels)
- **Input**: 256Ã—512 RGB images
- **Output**: 12-class semantic segmentation masks
- **Optimization**: Balanced accuracy and speed

### Fast Model  
- **Encoder**: EfficientNet-B0 (ImageNet pre-trained)
- **Decoder**: UNet architecture (reduced channels: [128, 64, 32, 16, 8])
- **Input**: 256Ã—512 RGB images
- **Output**: 12-class semantic segmentation masks
- **Optimization**: Optimized for maximum speed and real-time performance

## ğŸ“ File Structure

```
segmentationNeuralNetwork/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ demo_segmentation.py         # Universal testing/demo script (auto-detects model type)
â”œâ”€â”€ train_stable.py             # Stable training script (MobileNetV2)
â”œâ”€â”€ train_fast.py               # Fast training script (EfficientNet-B0)
â”œâ”€â”€ unity_dataset.py            # Unity data loader
â”œâ”€â”€ checkpoints_stable/         # Stable model checkpoints
â”‚   â”œâ”€â”€ best_model.pth          # Best stable model (download required)
â”‚   â””â”€â”€ last_model.pth          # Latest stable training checkpoint
â”œâ”€â”€ checkpoints_fast/           # Fast model checkpoints  
â”‚   â”œâ”€â”€ best_model.pth          # Best fast model (download required)
â”‚   â””â”€â”€ last_model.pth          # Latest fast training checkpoint
â”œâ”€â”€ DEMO_GUIDE.md               # Testing guide
â”œâ”€â”€ README_UNITY.md             # Unity-specific documentation
â””â”€â”€ README_REALTIME.md          # Real-time optimization guide
```

## ğŸ® Demo Features

The main demo script (`demo_segmentation.py`) provides:

- **6-panel Visualization**: Original, ground truth, prediction, overlay, errors, per-class IoU
- **Interactive Navigation**: Previous/Next/Random/Find Best/Worst buttons
- **Real-time Metrics**: FPS measurement and performance assessment
- **Batch Analysis**: Automated testing on multiple samples
- **Per-class Performance**: Detailed breakdown of class-wise accuracy

## ğŸ”§ Training

For training your own model:

```bash
# Stable training (high accuracy, balanced performance)
python train_stable.py

# Fast training (optimized for speed and real-time driving)
python train_fast.py

# Other training options
python train_high_accuracy.py
python train_realtime_model.py
```

## ğŸ“‹ Requirements

```bash
pip install torch torchvision
pip install segmentation-models-pytorch
pip install matplotlib numpy pillow
pip install opencv-python
```

## ğŸ¯ Unity Classes

The model recognizes 12 semantic classes:
- **Void** (0): Background/undefined areas
- **Building** (1): Structures and buildings  
- **Fence** (2): Barriers and fences
- **Other** (3): Miscellaneous objects
- **Pedestrian** (4): People and pedestrians
- **Pole** (5): Posts and poles
- **RoadLine** (6): Lane markings
- **Road** (7): Driveable surfaces
- **SideWalk** (8): Pedestrian walkways
- **Vegetation** (9): Trees and plants
- **Vehicles** (10): Cars and vehicles
- **Wall** (11): Walls and barriers

## ğŸ“š Additional Documentation

- [`DEMO_GUIDE.md`](DEMO_GUIDE.md) - Complete testing guide
- [`README_UNITY.md`](README_UNITY.md) - Unity-specific implementation details
- [`README_REALTIME.md`](README_REALTIME.md) - Real-time optimization guide
- [`ULTRA_TRAINING_GUIDE.md`](ULTRA_TRAINING_GUIDE.md) - Advanced training options

## âš¡ Performance Notes

Both models excel at structural detection (buildings, walls) and are suitable for real-time driving applications:

### Model Selection Guide:
- **Fast Model**: Choose for maximum speed (48.1 FPS) and real-time driving applications
- **Stable Model**: Choose for balanced accuracy and performance (47.0 FPS)

### Real-time Driving Assessment:
- âœ… **Both models exceed 30+ FPS** requirement for autonomous driving
- âœ… **Low latency**: ~20-21ms response time
- âœ… **Consistent performance**: Stable FPS with minimal variation
- âœ… **Production ready**: Both suitable for real-time deployment

For production use in real-time driving applications:
1. **âœ… Speed Requirement Met**: Both models achieve 47-48 FPS (target: 30+ FPS)
2. **Class Balance**: Additional training data for road/vehicle detection may improve accuracy
3. **Real-time Deployment**: Use the universal demo script for testing both models

## ğŸ¤ Contributing

When training new models or making improvements, ensure compatibility with the existing demo and testing infrastructure.
